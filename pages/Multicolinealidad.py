import numpy as np
import pandas as pd
import streamlit as st
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
import plotly.graph_objects as go
import plotly.figure_factory as ff

st.write(r'''
        La multicolinealidad es un fen√≥meno en la regresi√≥n lineal m√∫ltiple en el que dos o m√°s variables predictoras 
        est√°n altamente correlacionadas entre s√≠, lo que dificulta que el modelo distinga claramente el efecto individual 
        de cada variable sobre la variable dependiente. En t√©rminos m√°s espec√≠ficos, la multicolinealidad ocurre cuando 
        existe una relaci√≥n lineal fuerte entre algunas de las variables predictoras, y esto crea redundancia en la informaci√≥n.

        Efectos de la multicolinealidad en la regresi√≥n lineal:

        - Inestabilidad en los coeficientes: Cuando las variables predictoras est√°n altamente correlacionadas, los 
        coeficientes estimados de cada predictor se vuelven inestables. Peque√±os cambios en los datos pueden 
        provocar grandes variaciones en los coeficientes.

        - Menor significancia estad√≠stica: Debido a la redundancia en la informaci√≥n aportada por las variables 
        correlacionadas, el modelo tiene dificultades para distinguir el efecto individual de cada variable 
        sobre la variable dependiente, lo que reduce su significancia estad√≠stica.

        - Aumento en la varianza de los coeficientes estimados: Esto disminuye la precisi√≥n de los intervalos de confianza.

        - Dificultades para interpretar el modelo: Dado que es complicado separar los efectos individuales de cada 
        predictor, puede ser dif√≠cil interpretar los resultados y entender c√≥mo cada variable afecta realmente al resultado.

        Posibles soluciones en regresi√≥n lineal normal:

        - Eliminar variables: Si una variable no aporta informaci√≥n √∫nica, se puede considerar eliminarla del modelo.

        - Regularizaci√≥n: M√©todos como Ridge o Lasso penalizan los coeficientes altos, lo que puede ayudar a reducir el impacto de la multicolinealidad.

        - Transformaci√≥n de variables: Si las variables est√°n relacionadas de manera predecible, la transformaci√≥n puede reducir la multicolinealidad.

        - An√°lisis de componentes principales (PCA): El PCA puede convertir variables correlacionadas en un conjunto de nuevas variables 
        no correlacionadas, aunque pierde algo de interpretabilidad.

        Multicolinealidad en la regresi√≥n polin√≥mica

        La multicolinealidad se presenta en los modelos de regresi√≥n polin√≥mica debido a la correlaci√≥n intr√≠nseca entre las diferentes potencias 
        de una misma variable. Por ejemplo, en un modelo cuadr√°tico:
        
        $y = \beta_0 + \beta_1 x + \beta_2 x^2$
         
        los t√©rminos suelen estar correlacionados $x$ y $x^2$, ya que ambos dependen de la misma variable original X. A medida que el modelo 
        incluye t√©rminos de mayor grado, como X3 o X4 , esta correlaci√≥n aumenta, intensificando la multicolinealidad.
         
        2. Efectos de la multicolinealidad en la regresi√≥n polin√≥mica

        Seg√∫n el documento, los efectos de la multicolinealidad en la regresi√≥n polin√≥mica son:

        - Inestabilidad en los coeficientes estimados: La presencia de multicolinealidad hace que el c√°lculo de los coeficientes a trav√©s del 
        m√©todo de m√≠nimos cuadrados se vuelva inestable, lo que significa que pueden cambiar significativamente ante peque√±as variaciones en los datos.

        - Varianza elevada de los coeficientes: La matriz ùëãùëáùëã, utilizada para calcular los coeficientes en la ecuaci√≥n ùëè=(ùëãùëáùëã)‚àí1ùëãùëáùëå, se vuelve 
        casi singular (o no invertible) cuando hay multicolinealidad. Esto provoca un aumento en la varianza de los coeficientes, generando 
        estimaciones menos precisas.

        - Sobreajuste: La multicolinealidad puede llevar al modelo a ajustarse en exceso a las fluctuaciones de los datos de entrenamiento, 
        generando un modelo que no generaliza bien a nuevos datos.

        Posibles soluciones en regresi√≥n polin√≥mica:

        - Regularizaci√≥n (Ridge o Lasso): La regularizaci√≥n es especialmente √∫til en la regresi√≥n polin√≥mica para reducir el impacto de los t√©rminos 
        correlacionados.
        
        - Reducci√≥n de t√©rminos: En lugar de incluir todos los t√©rminos hasta un grado n, se puede probar con un modelo de menor grado.
        
        - Escalado de variables: Al estandarizar los valores antes de elevarlos a potencias, se puede reducir el impacto de la multicolinealidad.
         
        - Transformaci√≥n de bases ortogonales: En vez de emplear t√©rminos polinomiales de x, se pueden usar bases ortogonales como los 
        polinomios de Chebyshev o de Legendre, que est√°n dise√±ados para minimizar la multicolinealidad en modelos polin√≥micos.
         
        En el paper Minimizing the Effects of Collinearity in Polynomial Regression #link http://www.eng.tau.ac.il/~brauner/publications/IandEC_36_4405_97.pdf

        - Mayor Precisi√≥n y Validez Estad√≠stica: La transformaci√≥n de los datos permiti√≥ la obtenci√≥n de correlaciones m√°s precisas y estad√≠sticamente v√°lidas. 
        Se demostr√≥ que al reducir la multicolinealidad, era posible utilizar un polinomio de mayor grado, lo que mejoraba la precisi√≥n del modelo.

        - Transformaci√≥n: La transformaci√≥n que lleva los valores de la variable independiente al rango [‚àí1,1] fue la m√°s efectiva en reducir la colinealidad. 
        Esta transformaci√≥n result√≥ en el n√∫mero de condici√≥n m√°s bajo y permiti√≥ ajustar un polinomio de mayor grado sin problemas de multicolinealidad.
         
        - Uso de Polinomios Ortogonales: Los polinomios ortogonales fueron √∫tiles para reducir los efectos de la colinealidad, ya que redujeron la interdependencia 
        entre los par√°metros del modelo, proporcionando intervalos de confianza m√°s claros. Esto ayud√≥ a identificar cu√°les t√©rminos en el polinomio eran 
        estad√≠sticamente necesarios.

        - L√≠mite de Orden del Polinomio: La relaci√≥n entre el error de truncamiento y el error natural proporcion√≥ un criterio para establecer el grado m√°ximo de 
        polinomio que pod√≠a justificarse estad√≠sticamente. Si el valor de esta relaci√≥n era mayor que 1, se justificaba el uso de un polinomio de orden superior; 
        de lo contrario, la precisi√≥n adicional no agregaba valor estad√≠stico al modelo.

         ''')


import statsmodels.api as sm

# Generaci√≥n de datos
np.random.seed(23)
X = np.linspace(0, 10, 100)
y = 10 * np.exp(-0.2 * (X - 1)**2) - 5 * np.exp(-0.5 * (X - 6)**2) + np.random.normal(0, 1, X.shape)

col1, col2 = st.columns(2)

with col2: 
    scatter_plot = go.Figure()
    scatter_plot.add_trace(go.Scatter(x=X, y=y, mode='markers', marker=dict(color='blue')))
    scatter_plot.update_layout(
        title="Scatter Plot of Data",
        xaxis_title="X",
        yaxis_title="Y",
        height = 500
    )

    st.plotly_chart(scatter_plot)

with col1:
    kde_plot = ff.create_distplot([y], ["Density"], show_hist=False, show_rug=False)
    kde_plot.update_layout(title="Sapa",height = 500,showlegend = False)
    st.plotly_chart(kde_plot)

# Separar los datos en entrenamiento y prueba
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)
x_train = x_train.reshape(-1, 1)
x_test = x_test.reshape(-1, 1)

def polynomial_regression(degree):
    X_new = np.linspace(0, 10, 100).reshape(-1, 1)
    poly_features = PolynomialFeatures(degree=degree, include_bias=False)
    std_scaler = StandardScaler()
    lin_reg = LinearRegression()
    polynomial_regression = Pipeline([
        ("poly_features", poly_features),
        ("std_scaler", std_scaler),
        ("lin_reg", lin_reg),
    ])
    polynomial_regression.fit(x_train, y_train)
    y_new = polynomial_regression.predict(X_new)

    X_train_poly = poly_features.fit_transform(x_train)  # Transformar X_train a la forma polin√≥mica
    X_train_with_intercept = sm.add_constant(X_train_poly)  # Agregar constante para OLS
    ols_model = sm.OLS(y_train, X_train_with_intercept).fit()
    
    # Gr√°fica de regresi√≥n polin√≥mica con Plotly
    poly_plot = go.Figure()
    poly_plot.add_trace(go.Scatter(x=X_new.flatten(), y=y_new, mode='lines', name="Degree " + str(degree), line=dict(color='red')))
    poly_plot.add_trace(go.Scatter(x=x_train.flatten(), y=y_train, mode='markers', name="Train Data", marker=dict(color='blue')))
    poly_plot.add_trace(go.Scatter(x=x_test.flatten(), y=y_test, mode='markers', name="Test Data", marker=dict(color='green')))
    poly_plot.update_layout(
        title=f"Polynomial Regression (Degree {degree})",
        xaxis_title="X",
        yaxis_title="y"
    )
    st.plotly_chart(poly_plot)

    st.write(ols_model.summary())
# Selecci√≥n del grado en Streamlit
degree = st.selectbox("Select the degree for Polynomial Regression", [i for i in range(1, 26)])
polynomial_regression(degree)


import numpy as np
import plotly.graph_objs as go
import streamlit as st
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
from scipy.special import chebyt


# Dividir los datos en entrenamiento y prueba
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)
x_train = x_train.reshape(-1, 1)
x_test = x_test.reshape(-1, 1)

# Crear y entrenar el modelo de regresi√≥n con polinomios de Chebyshev
def chebyshev_regression(degree):
    X_new = np.linspace(0, 10, 100).reshape(-1, 1)

    # Generar caracter√≠sticas de Chebyshev
    chebyshev_features_train = np.column_stack([chebyt(d)(x_train.flatten()) for d in range(degree + 1)])
    chebyshev_features_test = np.column_stack([chebyt(d)(x_test.flatten()) for d in range(degree + 1)])
    chebyshev_features_new = np.column_stack([chebyt(d)(X_new.flatten()) for d in range(degree + 1)])
    
    # Ajustar el modelo
    model = Pipeline([
        ("scaler", StandardScaler()),
        ("lin_reg", LinearRegression())
    ])
    model.fit(chebyshev_features_train, y_train)
    y_new = model.predict(chebyshev_features_new)

    # Obtener los coeficientes del modelo
    coef = model.named_steps['lin_reg'].coef_

    # Agregar una constante para statsmodels (t√©rmino independiente)
    X_train_with_intercept = sm.add_constant(chebyshev_features_train)
    
    # Ajuste de modelo usando statsmodels para obtener el resumen
    ols_model = sm.OLS(y_train, X_train_with_intercept).fit()

    # Gr√°fico de la regresi√≥n
    poly_plot = go.Figure()
    poly_plot.add_trace(go.Scatter(x=X_new.flatten(), y=y_new, mode='lines', name="Degree " + str(degree), line=dict(color='red')))
    poly_plot.add_trace(go.Scatter(x=x_train.flatten(), y=y_train, mode='markers', name="Train Data", marker=dict(color='blue')))
    poly_plot.add_trace(go.Scatter(x=x_test.flatten(), y=y_test, mode='markers', name="Test Data", marker=dict(color='green')))
    poly_plot.update_layout(
        title=f"Chebyshev Polynomial Regression (Degree {degree})",
        xaxis_title="X",
        yaxis_title="y"
    )
    st.plotly_chart(poly_plot)

    # Mostrar resumen en Streamlit
    st.write(ols_model.summary())


# Selecci√≥n del grado en Streamlit
degree = st.selectbox("Select the degree for Chebyshev Polynomial Regression", [i for i in range(1, 26)])
chebyshev_regression(degree)

